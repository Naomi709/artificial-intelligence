{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c59860-96c4-432b-bcdd-38da3b7e5b2f",
   "metadata": {},
   "source": [
    "Q1: Define algorithmic bias and provide two examples.\n",
    "Definition: Algorithmic bias occurs when AI systems produce unfair outcomes due to prejudiced data or flawed model design.\n",
    "Examples:\n",
    "Hiring tools penalizing women due to historical male-dominated resumes.\n",
    "Facial recognition misidentifying minority groups.\n",
    "\n",
    "Q2: Transparency vs. Explainability in AI.\n",
    "Transparency: Openness about how an AI system works (disclosure of data sources, model structure).\n",
    "Explainability: Ability to interpret and understand why the AI made a specific decision.\n",
    "Why Important: Transparency builds trust; explainability ensures accountability and regulatory compliance.\n",
    "\n",
    "Q3: GDPR impact on AI in the EU.\n",
    "AI systems must comply with data protection principles (consent, data minimization).\n",
    "Right to explanation: Users can demand reasons for automated decisions.\n",
    "GDPR restricts the use of personal data, enforcing privacy-by-design in AI development."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcaa32c2-38d9-4ee5-bae5-0f0c5b5680dc",
   "metadata": {},
   "source": [
    "2. Ethical Principles Matching\n",
    "\n",
    "Principle\tDefinition\n",
    "B) Non-maleficence\tEnsuring AI does not harm individuals or society.\n",
    "C) Autonomy\tRespecting users’ right to control their data and decisions.\n",
    "D) Sustainability\tDesigning AI to be environmentally friendly.\n",
    "A) Justice\tFair distribution of AI benefits and risks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4095021-d225-4e37-954d-9c1376f17e8d",
   "metadata": {},
   "source": [
    "#### AI Fairness 360 - COMPAS Dataset Audit Template\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e58581ac-3fcd-4df1-a6da-a340b2b83fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: aif360 in c:\\users\\karis\\anaconda3\\envs\\edgeai\\lib\\site-packages (0.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\karis\\anaconda3\\envs\\edgeai\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\karis\\anaconda3\\envs\\edgeai\\lib\\site-packages (3.9.4)\n",
      "Requirement already satisfied: seaborn in c:\\users\\karis\\anaconda3\\envs\\edgeai\\lib\\site-packages (0.13.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement scikit-lear (from versions: none)\n",
      "ERROR: No matching distribution found for scikit-lear\n"
     ]
    }
   ],
   "source": [
    "!pip install aif360 pandas matplotlib seaborn scikit-lear"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e577fa-97ab-4004-b861-46121d1836d6",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from aif360.datasets import CompasDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.algorithms.preprocessing import Reweighing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#### Load COMPAS dataset\n",
    "dataset = CompasDataset()\n",
    "\n",
    "#### Define privileged and unprivileged groups (e.g., race)\n",
    "privileged_groups = [{'race': 1}]  # Caucasian\n",
    "unprivileged_groups = [{'race': 0}]  # African-American\n",
    "\n",
    "#### Bias Metrics Before Mitigation\n",
    "metric = BinaryLabelDatasetMetric(\n",
    "    dataset,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"### Bias Metrics Before Mitigation ###\")\n",
    "print(\"Disparate Impact:\", metric.disparate_impact())\n",
    "print(\"Statistical Parity Difference:\", metric.statistical_parity_difference())\n",
    "\n",
    "#### Visualize class balance\n",
    "labels = ['Low Risk', 'High Risk']\n",
    "counts = dataset.labels.sum(axis=0)\n",
    "plt.bar(labels, counts)\n",
    "plt.title('Recidivism Class Distribution')\n",
    "plt.show()\n",
    "\n",
    "#### Mitigation using Reweighing\n",
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                privileged_groups=privileged_groups)\n",
    "dataset_transf = RW.fit_transform(dataset)\n",
    "\n",
    "#### Bias Metrics After Mitigation\n",
    "metric_transf = BinaryLabelDatasetMetric(\n",
    "    dataset_transf,\n",
    "    unprivileged_groups=unprivileged_groups,\n",
    "    privileged_groups=privileged_groups\n",
    ")\n",
    "\n",
    "print(\"### Bias Metrics After Mitigation ###\")\n",
    "print(\"Disparate Impact:\", metric_transf.disparate_impact())\n",
    "print(\"Statistical Parity Difference:\", metric_transf.statistical_parity_difference())\n",
    "\n",
    "#### Visualize weights after reweighing (Optional)\n",
    "weights = dataset_transf.instance_weights\n",
    "sns.histplot(weights)\n",
    "plt.title('Instance Weights After Reweighing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6bfa66-95ca-4300-bb2c-ac0a8430ff58",
   "metadata": {},
   "source": [
    "### Word Audit Report\n",
    "COMPAS Dataset Fairness Audit Report\n",
    "\n",
    "This audit evaluates racial bias in the COMPAS recidivism prediction dataset using IBM’s AI Fairness 360 toolkit. The dataset predicts the likelihood of criminal reoffending but has been criticized for unfair outcomes, particularly towards African-American defendants.\n",
    "\n",
    "Initial analysis identified clear disparities. Using the disparate impact ratio, results showed values significantly below 0.8, indicating adverse impact against African-Americans. Similarly, the statistical parity difference revealed that African-Americans were disproportionately classified as high-risk compared to Caucasian defendants. Visualizations confirmed a skewed distribution in high-risk predictions.\n",
    "\n",
    "To mitigate this bias, we applied the Reweighing algorithm, which adjusts instance weights during model training to balance outcomes across privileged and unprivileged groups. Post-mitigation metrics indicated improvements: the disparate impact ratio moved closer to 1.0, and statistical parity difference reduced, suggesting fairer treatment between races.\n",
    "\n",
    "Based on these findings, we recommend integrating debiasing strategies such as Reweighing into any COMPAS-based risk assessment models. Further, we suggest ongoing monitoring using fairness metrics like:\n",
    "\n",
    "Disparate Impact\n",
    "\n",
    "Equal Opportunity Difference\n",
    "\n",
    "False Positive Rate Difference\n",
    "\n",
    "In conclusion, our audit confirms racial bias in the original COMPAS dataset, which could result in unjustified harsher treatments of African-American individuals. Mitigation methods can significantly improve fairness but require continuous evaluation. AI models deployed in sensitive areas like criminal justice must undergo rigorous, transparent audits to prevent reinforcing systemic biases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eedcbb4-a18f-409e-9bf8-ccda7d5374c4",
   "metadata": {},
   "source": [
    "### Ethical AI Healthcare Policy\n",
    "Guidelines for Ethical AI Use in Healthcare\n",
    "Objective: Ensure patient safety, fairness, and transparency in AI-powered healthcare systems.\n",
    "\n",
    "1. Patient Consent Protocols\n",
    "Collect explicit, informed consent before using patient data for AI.\n",
    "\n",
    "Inform patients about AI involvement in diagnoses or treatment recommendations.\n",
    "\n",
    "Offer opt-out options where feasible.\n",
    "\n",
    "2. Bias Mitigation Strategies\n",
    "Use diverse datasets representing age, race, gender, and health conditions.\n",
    "\n",
    "Perform routine bias audits using metrics like disparate impact ratio.\n",
    "\n",
    "Apply mitigation techniques (e.g., reweighting, data balancing).\n",
    "\n",
    "Engage independent auditors for fairness verification.\n",
    "\n",
    "3. Transparency Requirements\n",
    "Ensure AI models are explainable and interpretable to medical staff.\n",
    "\n",
    "Document:\n",
    "\n",
    "Data sources\n",
    "\n",
    "Model limitations\n",
    "\n",
    "Known biases\n",
    "\n",
    "Publicly disclose model accuracy across demographic groups.\n",
    "\n",
    "Establish a clear feedback mechanism for error reporting.\n",
    "\n",
    "4. Continuous Monitoring & Review\n",
    "Regularly update AI systems based on latest medical research.\n",
    "\n",
    "Conduct post-deployment bias and performance checks.\n",
    "\n",
    "Involve ethical oversight committees in decision-making.\n",
    "\n",
    "Conclusion\n",
    "Adhering to these principles will protect patients, foster trust, and ensure AI complements rather than compromises healthcare services."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
